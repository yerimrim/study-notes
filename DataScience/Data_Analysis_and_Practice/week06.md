## 인공신경망

- 퍼셉트론: 뇌의 뉴런을 본떠 만든 인공 구조
    - Output = $\sum$  (weight * input)
    - 훈련: 원하는 출력값을 내보내도록 가중치를 조정해가는 작업
    - 입력값이 같더라도 가중치를 조절하면 출력값이 바뀐다.
    - 단순 퍼셉트론은 선형 분류 문제밖에 풀지 못한다는 한계
    ⇒ 비선형 분류 문제를 풀기 위해 다층 퍼셉트론 사용
- 신경망: 입력층 + 은닉층 + 출력층
- 활성화 함수 : 입력값을 최종적으로 어떤 값으로 변환해 출력할지 결정하는 함수
    - Sigmoid 함수: 양수 = 1, 음수 = 0 출력 (S자 곡선)
    - ReLU 함수: (많이 쓰임)
    입력값>0 ⇒ 입력값 그대로 출력
    입력값≤0 ⇒ 0 출력
    - Leaky ReLU 함수: (많이 쓰임)
    입력값>0 ⇒ 입력값 그대로 출력
    입력값≤0 ⇒ 약간의 음숫값을 살려둠 ($0.01x$로 출력)
- 손실함수: 모델 성능이 얼마나 나쁜지를 측정하는 함수 
(작을수록 좋음)
    - MSE (회귀)
    - 교차 엔트로피 (분류)
- 경사하강법: 기울기가 0이 될 때까지(손실함수가 최소가 될 때까지) 기울기 아래 방향으로 이동
    - Learning Rate($\eta$): 얼만큼 이동할지 결정하는 값
    - 확률적 경사하강법: 전체 학습 데이터에서 **개별 데이터를 무작위**
    로 뽑아 경사 하강법을 수행
    - 미니배치 경사하강법: 데이터를 하나씩 훈련하기보다는 **여러 묶음으로 무작위로 묶어** 경사 하강법을 수행
- 순전파, 역전파

---

## 합성곱 신경망(CNN)

: Computer Vision에서 주로 사용되는 신경망

### 합성곱

- 합성곱(convolution): 차원 데이터의 일정 영역 내 값들을 하나의 값으로 압축한 연산
- 합성곱 계층(convolutional layer): 합성곱으로 이루어진 신경망 계층
- 필터(filter): 입력 데이터에서 특정한 특성을 필터링하는 역할 (Input * Filter = 합성곱)
- Feature Map: 합성곱 연산으로 얻은 결과
- 패딩(padding): 입력 데이터 주변을 특정 값으로 채우는 것 (보통 0으로 채움)
- 스트라이드(stride): 합성곱 연산을 수행할 때, 필터가 한 번에 이동하는 간격
- 풀링(pooling): 특성 맵 크기를 줄여 이미지의 요약 정보를 추출
    - 최대 풀링: 풀링 영영의 최댓값을 취함
    - 평균 풀링: 풀링 영역의 평균을 취함
    - 위치 불변성(location invariance): 풀링이 특정 영역의 요약 정보(대푯값)를 가져오므로 “위치가 변해도 같은 물체로 판단”하는 것
- **합성곱 신경망 전체 구조**
    
    

### 전결합

- 전결합(fully-connected): 이전 계층의 모든 노드 각각이 다음 계층의 노드 전부와 연결된 결합
- 전결합 계층 or 밀집 계층(fully-connected layer or dense layer): 전결합으로 구성된 계층
- 평탄화: 다차원 데이터를 1차원 데이터로 바꾸는 작업

---

## 성능 향상 알고리즘

- **드롭아웃**: 과대적합을 방지하기 위하여 신경망 훈련 과정에서 무작위로 일부 뉴런을 제외하는 기법
- **배치 정규화**: 과대적합 방지와 훈련 속도 향상을 위한 기법, 배치 단위로 정규화
    1. 정규화: 미니배치를 평균이 0, 분산이 1이 되게 정규화
    2. 스케일 조정 및 이동: 정규화한 데이터의 스케일을 조정하고, 이동시킴
- **옵티마이저**: 신경망의 최적 가중치를 찾아주는 알고리즘
    - **Momentum**: 물리학의 관성 개념을 추가한 옵티마이저
    - **Adagrad**: 최적 파라미터에 도달할수록 학습률을 낮추도록 한 옵티마이저
    - **RMSProp**: Adagrad의 단점을 보완한 방법
    ⇒ Adagrad는 훈련 시작 단계부터 기울기를 누적해 학습률을 낮추지만, 
    ⇒ RMSProp은 최근 기울기만 고려해 학습률을 낮춤
    ⇒ 훈련을 오래 지속해도 학습률이 0에 수렴하지 않음
    - **Adam**(가장 많이 사용): Momentum과 RMSProp의 장점을 결합한 방법, RMSProp처럼 적응적 학습률을 적용
- **전이학습**:  한 영역에서 사전 훈련된 모델(pretrained model)에 약간의 추가 학습을 더해 유사한 다른 영역에서도 활용하는 기법
