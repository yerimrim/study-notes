<aside>
ğŸ’¡

### Principle of locality

**1. Temporal locality (ìµœê·¼ì— ì‚¬ìš©ëœ ë°ì´í„°ê°€ ê³§ ë‹¤ì‹œ ì‚¬ìš©ë  ê°€ëŠ¥ì„±ì´ ë†’ë‹¤ëŠ” ì›ì¹™)**

: Recent past

after reading a book, â€œIt is important and Iâ€™ll use this book again.â€ so store on the desk, not return back. 

(What I experienced)

**2. Spatial locality (í˜„ì¬ ì ‘ê·¼í•œ ë°ì´í„° ê·¼ì²˜ì— ìˆëŠ” ë°ì´í„°ë„ ê³§ ì‚¬ìš©ë  ê°€ëŠ¥ì„±ì´ ìˆë‹¤ëŠ” ì›ì¹™)**

: Near one another

â€œmaybe Iâ€™ll need this book in the futureâ€, so bring that book.
what I usually use is stored in â€˜cache memoryâ€™, not in â€˜main memoryâ€™. 

(What I didnâ€™t experienced and just expecting)

</aside>

<aside>
ğŸ’¡

### Instruction

**1. Static Instruction** 

: exists in the code to be executed.

**2. Dynamic Instruction**

: appear in the execution trace of a program.

- í”„ë¡œê·¸ë¨ ê°œë°œì‹œ: Static Instruction
- í”„ë¡œê·¸ë¨ ì‹¤í–‰ì‹œ: Dynamic Instruction

**Code Reuse** is efficient when a small number of static instructions are reponsible for a large number of dynamic instructions.

</aside>

<aside>
ğŸ’¡

### Characteristics of memory systems

**[Key characteristics of computer memory systems]**

**1. Location**

- Internal (e.g. Processor registers, Cache, Main memory)
- External (e.g. Optical disks, Magnetic disks, Tapes)

**2. Capacity** (Stroage ; how much it can store, like GB, TB, and so on.)

- Number of words
- Number of bytes

**3. Unit of transfer** 

- Word
- Block

**4. Access Method**

- Sequential access (ìˆœì°¨ì ì¸ ì•¡ì„¸ìŠ¤)
    - : ê¸°ë¡ ë‹¨ìœ„ë¡œ ë©”ëª¨ë¦¬ ì ‘ê·¼
    - linear sequence
    - Access time - variable
- Direct access
    - : specific memory location address
    - Access time - variable
- Random access
    - : randomly select a location and the location directly addressed and accessed. (â€™Main memoryâ€™, some â€˜cache systemsâ€™ use this access method)
    - Access time - constant
- Associative access : Instead of using address, take a portion of the words.
    - : using a portion of its contents (not using address, but each location has its own addressing mechanism.) (â€™cache memoriesâ€™ use this access)
    - Retrieval time - constant

**5. Performance**

- Access time
    - : For random-access memory, It is the time to perform read or write operation.
    - : For non-random-access memory, It is the time to position the read-write mechanism at the desired loaction.
- Cycle time
    - : Current access starting time ~ Next access starting time
    - concerned with the â€˜system busâ€™, not the processor.
- Transfer rate
    - The rate of transferring data into or out of a memory unit.
    - For random-access memory, (Transfer rate) = 1/(cycle time)

**6. Physical Type**

- Semiconductor (ë°˜ë„ì²´)
    - Volatile or Nonvolatile
- Magnetic
    - Nonvolatile
- Optical (ìŒí–¥ ì‹ í˜¸ ì „ì†¡ ë‹¨ì)
- Magneto-optical

**7. Physical characteristics**

- Volatile (RAM)
    - : íœ˜ë°œì„±(ì „ì›ì´ ëŠì–´ì§€ë©´ ì •ë³´ë„ ìƒì–´ë²„ë¦¬ëŠ” ê²ƒ)
    - : information decays or is lost when electrical power is switched off.
- Nonvolatile (Second memory)
    - : Once recorded, information remains.
    - donâ€™t need electrical power.
- Erasable
    - : Can be altered.
- Nonerasable (ROM: Read-Only Memory)
    - : Cannot be altered, except by destroying the storage unit.

**8.  Oragnization**

- Memory modules

*External memories have both erassible and non erasable types.

*Volatile, Nonvolatile depends on electric power.

*A processor has its own â€˜local memoryâ€™, `Registers`.

</aside>

<aside>
ğŸ’¡

### The memroy hierarchy

- **Cost and performance characteristics**

*Price : Performance(Speed) > Capacity

*Registers > Cache memories(L1 > L2 > L3 >â€¦) > Main memory > Second memory

As it goes to the Left side, 

- Cost is more expensive

- Capacity is smaller

- Access time will decrease

- Performance is higher

- The frequency of the access of memory by processor will increase.

As capacity of cache memory is increasing, the performance of the cache memory will be decreased. Then, the cost will be decreased.

- **The IBM z13 memory hierarchy**
    - L1 and L2 caches use SRAM and are private for each other.
    - L3 cache uses eDRAM and is shared by all eight cores within the PU chip.
    ****
- **Design principles for a memory hierarchy**
    - Locality (how fast we can find the data)
        - : makes effective use of memory hierarchy.
        - As locality is higher, hit ratio is higher.
    - Inclusion (í•˜ìœ„ ë©”ëª¨ë¦¬ì— ì €ì¥ëœ ë°ì´í„°ê°€ ìƒìœ„ ë©”ëª¨ë¦¬ì—ë„ ì €ì¥ë˜ì–´ ìˆìŒ)
        - ë” ì‘ì€ L1 ìºì‹œì— ìˆëŠ” ë°ì´í„°ëŠ” ë” í° L2 ìºì‹œì—ë„ í¬í•¨ë˜ì–´ ìˆìŒ.
    - Coherence (ë³µì‚¬ë³¸ë“¤ì´ í•­ìƒ ì¼ê´€ëœ ê°’ì„ ìœ ì§€)
        - When a data is modified, copies must be updated immediately.
        - Vertical coherence (ìˆ˜ì§ ì¼ê´€ì„±, L1, L2, L3 ê°„ì˜ ì¼ê´€ì„± ìœ ì§€)
        - Horizontal coherence (ìˆ˜í‰ ì¼ê´€ì„±, L1 ë‚´ì˜ ë°ì´í„°ë“¤ì˜ ì¼ê´€ì„± ìœ ì§€)

</aside>

<aside>
ğŸ’¡

### Performance modeling of a multilevel memory hierarchy

**<Two-level memory access>**

: with `Cache` and `Main memory`

- M1: capacity and size are smaller, faster, more expensive (Cache memory)
- M2: capacity and size are larger, slower, cheaper (Main memory)
- M1 is used as temporary store for part of the contents of M2.
- Access time: Cache 1 < Cache 2 < Cache3 < Cache4 < â€¦ < Main memory
- Capacity: Cache1 < Cache2 < Cache3 < Cache4 < â€¦ < Main memory

- (Figure 4.8) Two-level memory accessì—ëŠ” M1ë§Œ access í•˜ëŠ” ê²½ìš°, M1, M2 ëª¨ë‘ ì‚¬ìš©í•˜ëŠ” ê²½ìš°ë¡œ ë‚˜ë‰¨.
â‡’ M1ë§Œ ì‚¬ìš©í•˜ëŠ” ê²½ìš°ê°€ ì¦ê°€í• ìˆ˜ë¡, average total access timeì´ access time of M1ì— ê°€ê¹Œì›€.
- locality can be exploited in two-level memory access
    - **locality**Â ë•ë¶„ì— M1ìœ¼ë¡œ ê°€ì ¸ì˜¨ ë°ì´í„°ê°€ ì—¬ëŸ¬ ë²ˆ ì‚¬ìš©ë  ìˆ˜ ìˆê³ , ì´ë¥¼ í†µí•´Â **ë¹ ë¥¸ ì „ë°˜ì ì¸ ì ‘ê·¼**ì´ ê°€ëŠ¥í•´ì§. â‡’ ë”°ë¼ì„œ, **locality**ê°€ two-level memory accessì—ì„œëŠ” ì„±ëŠ¥ì„Â **í–¥ìƒì‹œí‚¤ëŠ” ìš”ì¸**ìœ¼ë¡œ ì‘ìš©í•¨.
- (Figure 4.11)
    
    hit ratioë¥¼ ë†’ì´ê³  ì‹¶ìŒ.
    â‡’ ê·¸ëŸ¼ ê·¸ëƒ¥ ìºì‹œë©”ëª¨ë¦¬(M1)ì˜ í¬ê¸°ë¥¼ ë©”ì¸ë©”ëª¨ë¦¬(M2)ì˜ í¬ê¸°ì— ìµœëŒ€í•œ ê°€ê¹ê²Œ ëŠ˜ë¦¬ë©´ ë˜ëŠ”ê±° ì•„ëƒ?
    â‡’ ê·¸ëŸ¼ ë‹¹ì—°íˆ hit ratioëŠ” ëŠ˜ì–´ë‚˜ê² ì§€ë§Œ, ë¹„íŠ¸ë‹¹ í‰ê· ê°€ê²©ì´ ìƒìŠ¹í•œë‹¤.
    â‡’ ì •ë¦¬í•˜ë©´ hit ratio ë†’ì´ê² ë‹¤ê³  ë¬´ì‘ì • ìºì‹œë©”ëª¨ë¦¬ í¬ê¸°ë¥¼ ëŠ˜ë¦¬ëŠ” ê±´ ê²½ì œì ì´ì§€ ì•Šë‹¤.
    => ë”°ë¼ì„œ, ë©”ëª¨ë¦¬ì— optimalí•œ ì†”ë£¨ì…˜ì€ ì—†ë‹¤. ìºì‹œë©”ëª¨ë¦¬ë¥¼ ë©”ì¸ë©”ëª¨ë¦¬ í¬ê¸°ì— ë§ì¶°ì„œ ì ì  ëŠ˜ë¦¬ë©´ ë¹„íŠ¸ë‹¹ í‰ê· ê°€ê²©ì´ ìƒìŠ¹í•˜ë¯€ë¡œ ì ì ˆí•œ ê· í˜•ì„ ì´ë£¨ì–´ì•¼ í•œë‹¤.
    

___________________________________________________

ì •ë¦¬!!!!

- hit ratio: r = T2/T1 (T1: the time to access M1, T2: the time to access M2)
    
    T2 = 1, T1 =1 â†’ r = 1 (Access time of M1 and M2 is same)
    
    (low hit ratio â†’ access time decreases â†’ high access efficiency)
    
    T2 = 100, T1 = 1 â†’ r = 100 (high hit ratio â†’ access time increases â†’ low access efficiency)
    
    ~~T1 = 100, T2 =1 â†’ r = 0.01 (Always T1 < T2, so there not exists this case)~~
    
- If T2 and T1 are almost same, then hit ratio is low, and then the access efficiency is high.
- The gap of access time between T1 and T2 is increasing, then hit ratio is high, and then the access efficiency is low.
- Hit ratio is high â†’ access time increases â†’ low access efficiency â†’ low computer performance
- **Operation of Two-Level Memory access**
    - First, access to Cache1. If we canâ€™t find the data in Cache1 , access to Cache2 , to Cache3, â€¦, to Main memory.
    - When we found the data in Main memory, copy it from Main memory to Cache4 to Cache3 to Cache2 to Cache1.

hit ratio is too small, the size of the cache memory is small

hit ratio is too big, the access time is too long

**<Multilevel memory access>**
: with `Cache`, `Main memory`, `Second memory`

</aside>
